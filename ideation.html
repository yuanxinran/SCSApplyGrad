<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <meta name="description" content="" />
    <meta name="author" content="Template Mo" />
    <link
      href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700,900"
      rel="stylesheet"
    />

    <title>SCS ApplyGrad Website</title>

    <!-- Additional CSS Files -->
    <link
      rel="stylesheet"
      type="text/css"
      href="assets/css/bootstrap.min.css"
    />
    <link rel="stylesheet" type="text/css" href="assets/css/font-awesome.css" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/ekko-lightbox/5.3.0/ekko-lightbox.css"
      integrity="sha512-Velp0ebMKjcd9RiCoaHhLXkR1sFoCCWXNp6w4zj1hfMifYB5441C+sKeBl/T/Ka6NjBiRfBBQRaQq65ekYz3UQ=="
      crossorigin="anonymous"
    />
    <link
      rel="stylesheet"
      type="text/css"
      href="assets/css/templatemo-art-factory.css"
    />

    <link rel="stylesheet" type="text/css" href="assets/css/pages.css" />
    <!-- <link rel="stylesheet" type="text/css" href="assets/css/owl-carousel.css" /> -->
  </head>

  <body>
    <!-- ***** Header Area Start ***** -->
    <div id="header"></div>
    <!-- ***** Header Area End ***** -->

    <!-- ***** Welcome Area Start ***** -->
    <div class="page-welcome" id="process-header">
      <!-- ***** Header Text Start ***** -->
      <div class="header-text">
        <div class="container">
          <div class="row justify-content-center">
            <div
              class="left-text col-lg-10 col-md-10 col-sm-12 col-xs-12"
              data-scroll-reveal="enter left move 30px over 0.5s after 0.4s"
            >
              <h1>Ideation & Testing</h1>
              <p>
                What are possible opportunities we can explore and test out with
                reviewers?
              </p>
            </div>
          </div>
        </div>
      </div>
      <!-- ***** Header Text End ***** -->
    </div>
    <!-- ***** Welcome Area End ***** -->

    <!-- ***** Section 1 Start ***** -->
    <section class="section page-section" style="padding-bottom: 40px">
      <div class="container">
        <!-- ***** Subsection 1 ***** -->
        <!-- <div class="row justify-content-center">
          <div class="right-text col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
            <div class="left-heading section-header">
              <h4>Ideation</h4>
              <h6>
                What are possible opportunities we can explore and test out with
                Speed Dating?
              </h6>
            </div>
          </div>
        </div> -->
        <!-- ***** Subsection 1 End***** -->
        <!-- ***** Subsection 2 Start***** -->
        <div class="row justify-content-center section-subsection">
          <div class="col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
            <!-- <div class="tiny-title">RESEARCH METHOD</div> -->
            <p>
              With necessary background research done, we wonder: what are the
              some potential solutions that we can test out, not for the purpose
              of building them, but to get a deeper understanding of reviewers'
              attitudes towards certain aspects of the system such as fairness
              and machine learning? <br /><br />
              Our plan is to build and test with pretotype, a concept proposed
              by <a class="external-link" href="https://www.pretotyping.org/" target="_blank">Alberto Savoia</a>,
              Google's Innovation Agitator Emeritus​, to address the need of
              finding good innovation opportunities. For our pretotype, we
              wanted to explore boundaries for possible ML interventions,
              understand the similarities and differences across departments in
              evaluating applicants, and assess the impact of interventions at
              various stages of the admissions process. Our hypothesis was that
              interventions to help reviewers group, compare, and filter
              applicants quickly will help the efficiency and expedite the
              admissions process. <br /><br />
              We designed and presented five storyboards about problems in the
              admissions process and potential solutions around machine learning
              and automation. The participants, four reviewers representing 4
              different sized departments in the SCS, then co-designed and
              reflected on the solution’s implications.
            </p>
          </div>
        </div>
        <!-- ***** Subsection 2 End***** -->

        <!-- ***** Subsection 3 Start***** -->
        <div class="row justify-content-center section-subsection">
          <div class="col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
            <div class="tiny-title">RESEARCH QUESTIONS</div>
            <ul>
              <li>
                How comfortable are reviewers with incorporating ML into the
                admissions process? Which parts do they want to manually review?
              </li>
              <li>
                How can we help reviewers organize or sort through applicants
                without perpetuating bias?
              </li>
              <li>
                How do we create a solution that can be customized for the
                various needs of each department?
              </li>
            </ul>
            <p>
              <br />
              We conducted the <strong>Speed-Dating</strong> sessions with four
              masters program reviewers from four different departments: MHCI,
              MLT, MSAS, and MSPE. We intentionally recruited from these four
              departments to have a diversity in the size of applicants and
              students (the small departments are MSAS and MSPE with 0-199
              applicants, the medium department is MHCI with 200-499 applicants,
              and the large department is MLT with 500+ applicants).
              <br /><br />
              During the interview, we presented participants with several
              storyboards developed based on our previous research. A storyboard
              usually contains four frames: context, problem, solution, and
              resolution. For each storyboard, we pre-filled two frames for
              participants and asked them to fill the remaining two frames based
              on their admission experience. They could have multiple designs
              for each storyboard; for example, for storyboards with the empty
              ‘Problem’ frame, they could generate more than one problem for the
              context.
              <br /><br />
              After they filled in the corresponding frames, we prompted them to
              reflect on their experience as a reviewer and how their solution
              might impact the process. We asked them open-ended questions about
              any potential concerns they had, as well as asking them to use a
              Likert scale to rank how their solution would impact the
              effectiveness, efficiency, fairness of the admissions process.
            </p>
          </div>
          <div
            class="text-center col-lg-10 col-md-10 col-sm-10 mobile-top-fix image-large"
          >
            <a
              href="./assets/images/process/speed-dating-storyboard.png"
              data-toggle="lightbox"
              data-title="Admin Journey Map"
            >
              <img
                src="./assets/images/process/speed-dating-storyboard.png"
                class="img-fluid"
                alt="ApplyGrad Interface"
              />
            </a>

            <small class="image-subtitle"
              >Example storyboards for our speed dating</small
            >
          </div>
          <div class="col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
            <p>
              <br />
              For the most part, reviewers reacted negatively to ML taking part
              in the decision-making process and prefer it as a tool to check
              after they have made their own judgement calls. Through our
              storyboards, the need to develop a better note-taking system has
              been validated the most with grouping and filtering coming in
              next. It seems like the most needed design is something that can
              help reviewers remember applicant data.
              <br /><br />
              More specifically, our insights included:
            </p>
            <ul>
              <li>
                We should aim to develop tools that can facilitate helping
                reviewers take better notes for discussion.
              </li>
              <li>
                ApplyGrad could use some additional data and filter features
                that help people make decisions and calibrate reviews.
              </li>
              <li>
                We should not aim to use ML to make decisions for reviewers.
                Even when making recommendations, perhaps make it optional and
                keep in mind how it still may influence the order or way a
                person looks at information.
              </li>
              <li>
                We should use automation and ML at an acceptable level and
                ensure a human-in-the-loop decision making process.
              </li>
              <li>
                The designs should prioritize fairness over efficiency.
                Designing for fairness encompasses the need for efficiency.
                Tools and designs should aim to make weighing in fairness
                quicker, easier to read, and compare.
              </li>
            </ul>
          </div>
          <div
            class="text-center col-lg-10 col-md-10 col-sm-10 mobile-top-fix image-large"
          >
            <a
              href="./assets/images/process/speed-dating-insights.png"
              data-toggle="lightbox"
              data-title="Admin Journey Map"
            >
              <img
                src="./assets/images/process/speed-dating-insights.png"
                class="img-fluid"
                alt="ApplyGrad Interface"
              />
            </a>

            <small class="image-subtitle"
              >Speed dating insight summary board</small
            >
          </div>
        </div>

        <!-- ***** Subsection 3 End***** -->
      </div>
    </section>
    <!-- ***** Section 1 End ***** -->

    <!-- ***** Section 2 Start ***** -->
    <section class="section page-section" style="padding-bottom: 40px">
      <div class="container">
        <!-- ***** Subsection 1 ***** -->
        <div class="row justify-content-center">
          <div class="right-text col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
            <div class="left-heading section-header">
              <h4>Exploring One Idea</h4>
              <h6>
                How would reviewer react to a specific scenario of using machine
                learning?
              </h6>
            </div>
          </div>
        </div>
        <!-- ***** Subsection 1 End***** -->
        <!-- ***** Subsection 2 Start***** -->
        <div class="row justify-content-center section-subsection">
          <div class="col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
            <!-- <div class="tiny-title">RESEARCH METHOD</div> -->
            <p>
              Building off the reviewer's hesitation with machine learning
              interventions, we wanted to test our assumption that there was a
              way to introduce machine learning that wouldn’t have massive
              implications for bias while addressing a user need. We decided to
              make a conceptual prototype that could address the struggle to
              take notes and input reviews of each application into ApplyGrad,
              which takes a while and requires manual work for the reviewers and
              administrators.Our hypothesis was that a better review system
              within ApplyGrad that uses machine learning to do so can help
              expedite the review process without introducing new or additional
              bias into the admissions process.
            </p>
          </div>
        </div>
        <!-- ***** Subsection 2 End***** -->

        <!-- ***** Subsection 3 Start***** -->
        <div class="row justify-content-center section-subsection">
          <div class="col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
            <div class="tiny-title">RESEARCH QUESTIONS</div>
            <ul>
              <li>
                Are there commonalities between how people review within a
                program?
              </li>
              <li>
                Are there differences between how people review within a
                program?
              </li>
              <li>
                What are things that people pay attention to or want to
                highlight?
              </li>
            </ul>
            <p>
              <br />
              We employed a <strong>Wizard-of-Oz approach</strong> with Google
              Docs for our conceptual prototype. There were three applications
              for each reviewer to read, each consisting of a personal statement
              and resume. For the first application, the reviewer would think
              aloud as they went through the documents. They then moved on to
              the second application, where they used the normal Google Document
              functions of highlighting and commenting in their review. While
              the reviewer worked on the first two applications, the team took
              note of which pieces of information the reviewer deemed most
              important and highlighted analogous information in the third
              application. When the reviewer had completed their review of the
              first two applications, they began their review of the third
              application which had already been highlighted, thereby mimicking
              the function of a potential ML solution.
            </p>
          </div>
          <div
            class="text-center col-lg-10 col-md-10 col-sm-10 mobile-top-fix image-large"
          >
            <a
              href="./assets/images/process/conceptual-prototype-reviewer.png"
              data-toggle="lightbox"
              data-title="Admin Journey Map"
            >
              <img
                src="./assets/images/process/conceptual-prototype-reviewer.png"
                class="img-fluid"
                alt="ApplyGrad Interface"
              />
            </a>

            <small class="image-subtitle"
              >A sample statement purpose with reviewers' annotation</small
            >

            <a
              href="./assets/images/process/conceptual-prototype-annotated.png"
              data-toggle="lightbox"
              data-title="Admin Journey Map"
            >
              <img
                src="./assets/images/process/conceptual-prototype-annotated.png"
                class="img-fluid"
                alt="ApplyGrad Interface"
              />
            </a>

            <small class="image-subtitle"
              >"Automatically" annotated documents using wizard-of-oz</small
            >
          </div>
          <div class="col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
            <p>
              <br />
              We recruited 3 people from the HCI department to see if there were
              differences between reviewers within the same department. In
              addition, it was easier and quicker to generate fake data for a
              department that we were a part of due to FERPA regulations and
              time constraints. We only tested with three users and they are all
              from the MHCI program, so it is not a representative sample. In
              addition, HCII leans towards more holistic judgement rather than
              score-based, so it can not be generalized in this regard either.
              Due to time constraints, we also were unable to get through all
              three applications with each reviewer.
              <br /><br />
              If we decide to test this further, we will have to make the system
              extremely flexible and further explore collaborative highlighting
              between reviewers. We would also like to test the prototype with
              other departments and masters programs.
              <br /><br />
              Our general findings were that reviewers liked having control over
              the system and 2/3 started commenting and highlighting parts to
              themselves. They also liked how they could comment on the
              materials. Overall, reviewers diverged greatly in what they look
              for even with keywords. There was some concern about the
              highlighted information leading applicants to use “buzzwords” in
              their applications, but reviewers felt comfortable with factual
              information about their experience being highlighted.
              <br /><br /><br />More specifically, our insights included: <br />
            </p>
            <ul>
              <li>
                We can develop interfaces and interactions that make it easier
                to cross-reference similarities across application materials.
              </li>
              <li>
                Cross-referencing designs could help facilitate efficiency
                between reviewers and remembering information between admission
                meetings.
              </li>
              <li>
                Helping reviewers fill-in their gaps in knowledge for companies,
                internships, and classes could help them make better reviews.
              </li>
              <li>
                The solution we design should meet both the program’s needs for
                consistency and individual’s needs for flexibility.
              </li>
              <li>
                The tools we build must be robust to account for different
                keywords that people are looking for.
              </li>
              <li>
                In order to keep in account fairness, perhaps highlights can be
                part of a semi-blind check-in process.
              </li>
            </ul>
          </div>
          <div
            class="text-center col-lg-10 col-md-10 col-sm-10 mobile-top-fix image-large"
          >
            <a
              href="./assets/images/process/conceptual-prototype-insights.png"
              data-toggle="lightbox"
              data-title="Admin Journey Map"
            >
              <img
                src="./assets/images/process/conceptual-prototype-insights.png"
                class="img-fluid"
                alt="ApplyGrad Interface"
              />
            </a>

            <small class="image-subtitle"
              >Speed dating insight summary board</small
            >
          </div>
        </div>

        <!-- ***** Subsection 3 End***** -->
      </div>
    </section>
    <!-- ***** Section 2 End ***** -->

    <!-- ***** Bottom Navigation Start ***** -->
    <!-- <section class="section bottom-navigation">
      <div class="container"> -->
        <!-- ***** Section Title Start ***** -->
        <!-- <div class="row justify-content-between">
          <div class="col-lg-5">
            <a
              class="active"
              href="./process.html"
              style="position: absolute; right: 0px"
              ><i class="fa fa-chevron-left" style="font-size: 18px"></i>
              Previous: Process</a
            >
          </div>
          <div class="col-lg-5">
            <a class="active" href="./needs-identified.html"
              >Next: Needs Identified
              <i class="fa fa-chevron-right" style="font-size: 18px"></i
            ></a>
          </div>
        </div>
      </div>
    </section> -->
    <!-- ***** Bottom Navigation End ***** -->

    <!-- ***** Footer Start ***** -->
    <div id="footer"></div>

    <!-- jQuery -->
    <script src="assets/js/jquery-2.1.0.min.js"></script>

    <!-- load in header and footer -->
    <script>
      $(function () {
        $("#header").load("./header.html");
        $("#footer").load("./footer.html");
      });
    </script>

    <!-- Bootstrap -->
    <script src="assets/js/popper.js"></script>
    <script src="assets/js/bootstrap.min.js"></script>

    <!-- Plugins -->
    <script src="assets/js/owl-carousel.js"></script>
    <script src="assets/js/scrollreveal.min.js"></script>
    <script src="assets/js/waypoints.min.js"></script>
    <script src="assets/js/jquery.counterup.min.js"></script>
    <script src="assets/js/imgfix.min.js"></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/ekko-lightbox/5.3.0/ekko-lightbox.min.js"
      integrity="sha512-Y2IiVZeaBwXG1wSV7f13plqlmFOx8MdjuHyYFVoYzhyRr3nH/NMDjTBSswijzADdNzMyWNetbLMfOpIPl6Cv9g=="
      crossorigin="anonymous"
    ></script>

    <!-- Global Init -->
    <script src="assets/js/custom.js"></script>
  </body>
</html>
