<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="Template Mo" />
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700,900" rel="stylesheet" />

    <title>SCS ApplyGrad Website</title>

    <!-- Additional CSS Files -->
    <link rel="stylesheet" type="text/css" href="assets/css/bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="assets/css/font-awesome.css" />
    <link rel="stylesheet" type="text/css" href="assets/css/templatemo-art-factory.css" />

    <link rel="stylesheet" type="text/css" href="assets/css/pages.css" />
    <!-- <link rel="stylesheet" type="text/css" href="assets/css/owl-carousel.css" /> -->
</head>

<body>
    <!-- ***** Header Area Start ***** -->
    <div id="header"></div>
    <!-- ***** Header Area End ***** -->

    <!-- ***** Welcome Area Start ***** -->
    <div class="page-welcome" id="problem-header">
        <!-- ***** Header Text Start ***** -->
        <div class="header-text">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="left-text col-lg-10 col-md-10 col-sm-12 col-xs-12"
                        data-scroll-reveal="enter left move 30px over 0.5s after 0.4s">
                        <h1>Design Validation</h1>
                        <p>How did we ensure our design fits reviewers' needs?</p>
                    </div>
                </div>
            </div>
        </div>
        <!-- ***** Header Text End ***** -->
    </div>
    <!-- ***** Welcome Area End ***** -->




    <!-- ***** Section 1 Start ***** -->
    <section class="section page-section">
        <div class="container">
            <div class="row justify-content-center">
                <div class="right-text col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
                    <div class="left-heading">
                        <h4>Design Validation Through Various Activities</h4>
                    </div>
                    <div class="left-text">
                        <p>
                            A large part of our <a href="./design-process.html" class="external-link">sprint
                                planning</a> that involved active design was how to validate our design. From low-fi to
                            high-fi, different prototypes were used to probe distinct questions related to our users’
                            needs. Also, for our project, we are invested in ensuring making the design right as well as
                            making the right design. This required us to design a variety of activities to validate our
                            design and gain deeper knowledge of our user group.
                            <br><br>
                            As a result, our team came up with 4 design verification and validation methods, applied to
                            different rounds of user testing.
                        </p>
                        <br>
                        <div class="tiny-title">Contextual Interview </div>
                        <p>Contextual interview was the most-used validation activity we conducted throughout our
                            iterations. By conducting interviews and asking reviewers to use our system to review
                            applications, we were able to get honest feedback about our prototypes. For all the
                            prototypes we had, we asked reviewers to think aloud and conduct an application review.
                        </p>
                        <p>We looked for both people with and without previous review experience to look at our system
                            as reviewers and new users. In addition, for people with review experience, we didn’t limit
                            ourselves to faculty reviewers at universities, but also recruiters in the industry and PhD
                            students who also participated in PhD student reviews.
                        </p>
                        <p>
                            However, the number of interviews we could conduct in a 2-week sprint was limited by the
                            availability of our teammates. In order to maximize the feedback we could get, we also used
                            a platform called <a href="https://maze.co" class="external-link">maze.co</a> to conduct
                            automated and unmoderated testing sessions by
                            pre-defining tasks interviewees will go through. Our participants will go on maze to finish
                            the set of tasks in our system on their own time, and maze will help us collect much
                            quantitative and qualitative data. Some data from maze such as the error rate of clicks in
                            our prototype is hard to gather during traditional testing sessions.
                        </p>
                        <br>
                        <div class="tiny-title">Cohort cross-critique</div>
                        <p>
                            As an additional part of our learning process, during some of our capstone classes every
                            Tuesday and Thursday, we held cohort cross-critique with other capstone teams. Each session
                            lasted about 1 hour where we were paired with another capstone team to receive and provide
                            feedback.
                        </p>
                        <br>
                        <!-- TODO: Add image of cohort critique -->
                        <p>
                            During these sessions, we were able to talk with teams working with clients including
                            Bloomberg, NASA, SWAPPA. All the capstone projects were diverse in the topics, for example,
                            the three teams mentioned above were individually working on creating an internal system for
                            data scientists to train machine learning models, asynchronous communication between
                            astronauts and ground control, as well as helping people with accessibility needs navigate
                            around locations. Talking to teams working in a completely different domain brought us new
                            perspectives on our design.
                        </p>

                        <br>
                        <div class="tiny-title">Design Critique</div>
                        <p>
                            In addition to the cohort cross-critique, our team reached out to UX designers, graphic designers, as well as interface designers for design critique on our more refined prototypes. We conducted design critique on more defined prototypes, since it’s less useful for having designers look at our low-fi interfaces that are not fully decided. In all, we were able to get 6 designers and reviewers to look at our interfaces and provide valuable feedback for us.
                        </p>
                        <br>
                        <div class="tiny-title">Simulation committee meetings </div>
                        <p>
                            Another important piece of data we needed to validate our design is how much our system, mostly the annotation and comment component, can help reviewers have more effective and efficient conversations during admission meetings. Theoretically, by having highlights and annotations on raw application materials in ApplyGrad, reviewers can easily see important elements in application materials during the committee meetings and quickly recall reasons for evaluating an applicant in certain ways.
                        </p>
                        <p>
                            To figure out if this is really the case, we organized 4 simulation admission committee meetings with reviewers from programs including MHCI, METALS, MSE, and HCII PhD at SCS. We asked for real students from these programs for their application materials to recreate an authentic review experience. Before the committee meeting, we asked reviewers to review an application and put highlights and comments if they wish in the materials. Each application will be reviewed by at least two reviewers in the committee. Reviewers’ general comments and evaluations were recorded in committee review forms. A few days later, reviewers from the same committee came together on Zoom to discuss the assigned applications. A decision to admit the applicant or not was made at the end of the discussion. We conducted a short interview regarding reviewers’ experiences with the highlight and annotation system afterward.
                        </p>
                        <p>
                            In general, we gathered many positive comments from reviewers regarding how our design could help them in committee meetings. <strong>Reviewers on average scored our feature 4 out of 5 regarding its usefulness based on their interaction with it.</strong> The comments they left previously in the system greatly assisted them in recalling their evaluation, and seeing other reviewers’ notes helped them to identify potential areas of discussion during the meetings. Only 2 reviewers expressed they might not fit adding highlights and comments into their workflow purely for personal preferences – even in these cases, they felt positive about seeing other people’s notes.
                        </p>

                    </div>



                </div>
            </div>
        </div>
        </div>
    </section>
    <!-- ***** Section 1 End ***** -->


    <!-- ***** Section 2 Start ***** -->
    <section class="section page-section">
        <div class="container">
            <div class="row justify-content-center">
                <div class="right-text col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
                    <div class="left-heading">
                        <h4>3 Layers of Design Goals</h4>
                    </div>
                </div>
            </div>
            <div class="row justify-content-center">
                <div class="col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
                    <p>With the above 3 design directions we identified in our research, we defined the following three
                        levels of
                        goals that we want to achieve to guide our prototype design.
                    </p>
                </div>
            </div>
            <br><br>
            <div class="row justify-content-center">
                <div class="col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
                    <h5>Level 1: Designing for Efficiency</h5>
                    <p>
                        Throughout research, we discovered that to find evidence from different materials, reviewers
                        usually perform
                        what we called “swiveling”, i.e. switching back and forth between different materials to form a
                        comprehensive view of an applicant’s experiences. Current ApplyGrad interface didn’t support
                        this action,
                        which constantly breaked reviewers’ thought flow and contributed to longer review times and
                        lower
                        efficiency.
                    </p>
                    <br>
                    <p>Also, since the only way to record ideas in ApplyGrad is using the review form, there’s no way to
                        link
                        reviewers’ evaluation to any rational or evidence found in materials. As a result, after a few
                        weeks or
                        months, during the committee meeting where reviewers discuss applicants together, some reviewers
                        tend to
                        forget details why they evaluated applications in a certain way.
                    </p>
                    <br>
                    <p>According to reviewers in our interviews, this happens frequently and reviewers take time to go
                        back to the
                        raw materials to look up for evidence, which leads to inefficient meeting discussions.
                    </p>
                    <br>
                    <div class="col-lg-8 col-md-8 col-sm-8 row d-flex justify-content-end">
                        <div class="callout">
                            <div class="callout-content">
                                "You can see [during admission meetings] some reviewers forgot why they gave certain
                                evaluations and
                                started to do the review on the fly, which is not good."

                            </div>
                            <div class="callout-source">
                                –– SCS Reviewer during our prototype testing
                            </div>
                        </div>
                    </div>
                    <br>
                    <p>
                        Moreover, many reviewers are not even aware of functionalities existing in ApplyGrad such as
                        choosing what
                        columns to display in the data table. This is mostly a result of having too much information
                        displayed on
                        one interface in the old ApplyGrad, and reviewers naturally ignore many elements on the
                        periphery of the
                        pages. Not knowing and utilizing useful functionalities in ApplyGrad puts the efforts of
                        developing the
                        system in vain and add much extra work and friction for reviewers.
                    </p>
                    <br>
                    <div class="tiny-title">Reduce Swiveling</div>
                    <p>In order to reduce the need to swivel between materials and windows, instead of having the
                        reviewer to open
                        all application materials in different tabs, we merged all application materials into one PDF
                        document and
                        added a table of contents to support easier navigation. Also, we put the review form, usually
                        alongside to
                        keep all main user tasks within the system. This way, reviewers do not have to click deep into
                        an applicant
                        and back out to get to the next review.
                    </p>
                    <br>
                    <div class="tiny-title">Record of Evaluation Evidence</div>
                    <p>Reviewers hold a lot of information in their head when reviewing. It’s often hard to keep track
                        of, so they
                        use either external spreadsheets or physical notebooks to help them keep track. We wanted to
                        optimize their
                        time in the system by giving reviewers the functionality of highlighting, commenting, and
                        annotating.
                    </p>
                    <br>
                    <div class="tiny-title">Reduce Clutter in Display
                    </div>
                    <p>In our design, we focused on reducing unnecessary information and constructing progressive
                        disclosure for
                        elements displayed on different screens. Not only did this result in clear interfaces, it also
                        reduced the
                        possibility of reviewers being overwhelmed by the abundant of functionalities on one interface
                        and not using
                        many features we developed.
                    </p>


                    <!-- TODO: Insert Image here -->
                    <br>
                    <br>
                    <h5>Level 2: Designing for Effectiveness</h5>
                    <p>
                        There’s a significant difference between efficiency and effectiveness in the context of
                        application review
                        work. Efficiency emphasizes on the speed of doing review work, while effectiveness focuses on
                        the fairness
                        in review and how in-depth the review work is. It’s dangerous to singly pursue efficiency in
                        review, as the
                        work is inherently time-consuming and speeding through the materials will inevitably lead to a
                        shallow, not
                        comprehensive understanding of applicants’ backgrounds and experiences.
                    </p>
                    <br>
                    <p>
                        In our project, we prioritize effectiveness over efficiency, which is inline with the insights
                        we discovered
                        from our research. Many reviewers expressed that they are willing to spend a good amount of time
                        on an
                        application if in-detail assessment requires it. With this in mind, we included the following
                        features that
                        helps to make the review process more effective:
                    </p>
                    <br>

                    <div class="tiny-title">Reducing Cognitive Load</div>
                    <p>
                        Reviewers often hold a lot of information in their head when reviewing, which it’s often hard to
                        keep track
                        of. In our spring research, we found that many reviewers use either external spreadsheets or
                        physical
                        notebooks to help them keep track of their notes. We wanted to optimize their time in the system
                        by giving
                        reviewers the functionality of highlighting, commenting, and annotating.
                    </p>
                    <br>
                    <p>Reviewers said that they would be the most likely to use highlighting and annotation functions
                        for reviews
                        because it is similar to their actual workflow. People liked how how highlighting and annotating
                        is optional
                        and would not add any additional time to their review since it’s something that they already do.
                    </p>
                    <br>
                    <p>In addition, some reviewers also expressed interest in using different colors to distinguish
                        highlights
                        about different topics or added by different reviewers. </p>
                    <br>
                    <div class="tiny-title"> Visual Summaries
                    </div>
                    <p>Reviewers liked seeing notes compiled together. They often do it to recalibrate their reviews or
                        for
                        admission committee purposes, in which they can select different points to discuss about
                        candidates. In the
                        early version of our prototype, we tested how commenting and tagging would work, giving it
                        search
                        functionalities.
                    </p>
                    <br>
                    <!-- TODO: Add an image of visual summaries we have -->

                    <p>
                        A repeating theme for review work is that people often want to have easy access to their notes
                        and
                        annotations while doing final evaluations. Reviewers tend to store their notes in external
                        spreadsheets or
                        physical notebooks, and refer to them while filling in the review forms. Since our design has
                        reviewers put
                        notes directly linked to application materials in ApplyGrad, we automatically populate the notes
                        to the
                        review form field that each note is associated with. This supports a better evidence-based
                        review process as
                        reviewers’ evaluation fields in the review forms are now accompanied by their own annotations in
                        the
                        materials.

                    </p>
                    <br>
                    <br>
                    <h5>Level 3: Designing for Experience</h5>
                    <p>The current experience of using ApplyGrad is far from fun and enjoyable. Using our research data,
                        our team
                        constructed the following reviewer journey map to represent their experience in the system.
                    </p>

                    <!-- TODO: Image of journey map (in figma) -->
                    <p>
                        While trying to improve the usefulness of ApplyGrad, we specifically put effort into improving
                        the
                        experience of reviewers in our system, to make the process less stressful and straining, and
                        more rewarding.

                    </p>
                    <br>
                    <div class="tiny-title">A New Look – Updating Aesthetics for Anti-stress</div>
                    <p>One of the things we discovered from our research was that the interface of the original
                        ApplyGrad is very
                        cluttered and causes confusion for both new and old ApplyGrad users. We made improvements to the
                        interface
                        by reducing the clutter and introducing a cleaner design that makes it easier for reviewers to
                        process
                        information.
                    </p>
                    <br>
                    <!-- TODO: Add image of interface change in report -->
                    <br>
                    <br>
                    <div class="tiny-title">Anti-Stress – Adding Positive Feedback</div>
                    <p>Reviewers spend multiple hours reviewing applications for their respective programs, but don’t
                        get any
                        positive feedback from the system. From our research, we found that reviewers want to feel a
                        sense of
                        accomplishment when they completed a review. Thus, we explored the integration of features in
                        the system
                        that celebrate reviewers’ progress in the review process with a thank you message/celebration in
                        order to
                        give them a sense of accomplishment and a confirmation of their completed review.</p>

                    <!-- TODO: Add an image in the google doc -->
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12">
                    <div class="hr"></div>
                </div>
            </div>
        </div>
    </section>
    <!-- ***** Section 2 End ***** -->

    <!-- Figma interactive prototype -->
    <section class="section page-section">
        <div class="col-lg-12 col-md-12 col-sm-12 mobile-top-fix text-center">
            <h4>Interactive Figma Prototype</h4>
            <br>
            <p>Feel free to play around!</p>
            <br><br>
        </div>
        <div class="figma-prototype-container">
            <iframe class="figma-prototype"
                src="https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Fproto%2F65qL3JBKyNesUdpOb7x25X%2FApplyGrad-Sprint-4-Prototype-FINAL%3Fpage-id%3D1327%253A29682%26node-id%3D1327%253A30329%26viewport%3D744%252C2117%252C0.2334328144788742%26scaling%3Dmin-zoom%26starting-point-node-id%3D1327%253A29843"
                allowfullscreen></iframe>
        </div>
    </section>

    <!-- ***** Section 2 Start ***** -->
    <!-- <section class="section bottom-navigation">
    <div class="container"> -->
    <!-- ***** Section Title Start ***** -->
    <!-- <div class="row justify-content-between">
        <div class="col-lg-5">
          <a class="inactive" style="position: absolute; right: 0px"><i class="fa fa-chevron-left"
              style="font-size: 18px"></i>
            Previous</a>
        </div>
        <div class="col-lg-5">
          <a class="active" href="./research.html">Next: Research
            <i class="fa fa-chevron-right" style="font-size: 18px"></i></a>
        </div>
      </div>
    </div>
  </section> -->
    <!-- ***** Section 2 End ***** -->

    <!-- ***** Footer Start ***** -->
    <div id="footer"></div>

    <!-- jQuery -->
    <script src="assets/js/jquery-2.1.0.min.js"></script>

    <!-- load in header and footer -->
    <script>
        $(function () {
            $("#header").load("./header.html");
            $("#footer").load("./footer.html");
        });
    </script>

    <!-- Bootstrap -->
    <script src="assets/js/popper.js"></script>
    <script src="assets/js/bootstrap.min.js"></script>

    <!-- Plugins -->
    <script src="assets/js/owl-carousel.js"></script>
    <script src="assets/js/scrollreveal.min.js"></script>
    <script src="assets/js/waypoints.min.js"></script>
    <script src="assets/js/jquery.counterup.min.js"></script>
    <script src="assets/js/imgfix.min.js"></script>

    <!-- Global Init -->
    <script src="assets/js/custom.js"></script>
</body>

</html>