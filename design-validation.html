<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="Template Mo" />
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700,900" rel="stylesheet" />

    <title>SCS ApplyGrad Website</title>

    <!-- Additional CSS Files -->
    <link rel="stylesheet" type="text/css" href="assets/css/bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="assets/css/font-awesome.css" />
    <link rel="stylesheet" type="text/css" href="assets/css/templatemo-art-factory.css" />

    <link rel="stylesheet" type="text/css" href="assets/css/pages.css" />
    <!-- <link rel="stylesheet" type="text/css" href="assets/css/owl-carousel.css" /> -->
</head>

<body>
    <!-- ***** Header Area Start ***** -->
    <div id="header"></div>
    <!-- ***** Header Area End ***** -->

    <!-- ***** Welcome Area Start ***** -->
    <div class="page-welcome" id="problem-header">
        <!-- ***** Header Text Start ***** -->
        <div class="header-text">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="left-text col-lg-10 col-md-10 col-sm-12 col-xs-12"
                        data-scroll-reveal="enter left move 30px over 0.5s after 0.4s">
                        <h1>Design Validation</h1>
                        <p>How did we ensure our design fits reviewers' needs?</p>
                    </div>
                </div>
            </div>
        </div>
        <!-- ***** Header Text End ***** -->
    </div>
    <!-- ***** Welcome Area End ***** -->




    <!-- ***** Section 1 Start ***** -->
    <section class="section page-section">
        <div class="container">
            <div class="row justify-content-center">
                <div class="right-text col-lg-10 col-md-10 col-sm-12 mobile-top-fix">
                    <div class="left-heading">
                        <h4>Design Validation Through Various Activities</h4>
                    </div>
                    <div class="left-text">
                        <p>
                            A large part of our <a href="./design-process.html" class="external-link">sprint
                                planning</a> that involved active design was how to validate our design. From low-fi to
                            high-fi, different prototypes were used to probe distinct questions related to our users’
                            needs. Also, for our project, we are invested in ensuring making the design right as well as
                            making the right design. This required us to design a variety of activities to validate our
                            design and gain deeper knowledge of our user group.
                            <br><br>
                            As a result, our team came up with 4 design verification and validation methods, applied to
                            different rounds of user testing.
                        </p>
                        <br>
                        <div class="tiny-title">Contextual Interview </div>
                        <p>Contextual interview was the most-used validation activity we conducted throughout our
                            iterations. By conducting interviews and asking reviewers to use our system to review
                            applications, we were able to get honest feedback about our prototypes. For all the
                            prototypes we had, we asked reviewers to think aloud and conduct an application review.
                        </p>
                        <p>We looked for both people with and without previous review experience to look at our system
                            as reviewers and new users. In addition, for people with review experience, we didn’t limit
                            ourselves to faculty reviewers at universities, but also recruiters in the industry and PhD
                            students who also participated in PhD student reviews.
                        </p>
                        <p>
                            However, the number of interviews we could conduct in a 2-week sprint was limited by the
                            availability of our teammates. In order to maximize the feedback we could get, we also used
                            a platform called <a href="https://maze.co" class="external-link">maze.co</a> to conduct
                            automated and unmoderated testing sessions by
                            pre-defining tasks interviewees will go through. Our participants will go on maze to finish
                            the set of tasks in our system on their own time, and maze will help us collect much
                            quantitative and qualitative data. Some data from maze such as the error rate of clicks in
                            our prototype is hard to gather during traditional testing sessions.
                        </p>
                        <br>
                        <div class="tiny-title">Cohort cross-critique</div>
                        <p>
                            As an additional part of our learning process, during some of our capstone classes every
                            Tuesday and Thursday, we held cohort cross-critique with other capstone teams. Each session
                            lasted about 1 hour where we were paired with another capstone team to receive and provide
                            feedback.
                        </p>
                        <img src="./assets/images/design-validation/cohort-crit.png" alt="Cohort Critique" class="img-fluid">
                        <div class="text-center image-description">Sadly, this year's cohort critiques are mostly over Zoom :(</div>
                        <br>
                        <p>
                            During these sessions, we were able to talk with teams working with clients including
                            Bloomberg, NASA, SWAPPA. All the capstone projects were diverse in the topics, for example,
                            the three teams mentioned above were individually working on creating an internal system for
                            data scientists to train machine learning models, asynchronous communication between
                            astronauts and ground control, as well as helping people with accessibility needs navigate
                            around locations. Talking to teams working in a completely different domain brought us new
                            perspectives on our design.
                        </p>

                        <br>
                        <div class="tiny-title">Design Critique</div>
                        <p>
                            In addition to the cohort cross-critique, our team reached out to UX designers, graphic designers, as well as interface designers for design critique on our more refined prototypes. We conducted design critique on more defined prototypes, since it’s less useful for having designers look at our low-fi interfaces that are not fully decided. In all, we were able to get 6 designers and reviewers to look at our interfaces and provide valuable feedback for us.
                        </p>
                        <br>
                        <div class="tiny-title">Simulation committee meetings </div>
                        <p>
                            Another important piece of data we needed to validate our design is how much our system, mostly the annotation and comment component, can help reviewers have more effective and efficient conversations during admission meetings. Theoretically, by having highlights and annotations on raw application materials in ApplyGrad, reviewers can easily see important elements in application materials during the committee meetings and quickly recall reasons for evaluating an applicant in certain ways.
                        </p>
                        <p>
                            To figure out if this is really the case, we organized 4 simulation admission committee meetings with reviewers from programs including MHCI, METALS, MSE, and HCII PhD at SCS. We asked for real students from these programs for their application materials to recreate an authentic review experience. Before the committee meeting, we asked reviewers to review an application and put highlights and comments if they wish in the materials. Each application will be reviewed by at least two reviewers in the committee. Reviewers’ general comments and evaluations were recorded in committee review forms. A few days later, reviewers from the same committee came together on Zoom to discuss the assigned applications. A decision to admit the applicant or not was made at the end of the discussion. We conducted a short interview regarding reviewers’ experiences with the highlight and annotation system afterward.
                        </p>
                        <p>
                            In general, we gathered many positive comments from reviewers regarding how our design could help them in committee meetings. <strong>Reviewers on average scored our feature 4 out of 5 regarding its usefulness based on their interaction with it.</strong> The comments they left previously in the system greatly assisted them in recalling their evaluation, and seeing other reviewers’ notes helped them to identify potential areas of discussion during the meetings. Only 2 reviewers expressed they might not fit adding highlights and comments into their workflow purely for personal preferences – even in these cases, they felt positive about seeing other people’s notes.
                        </p>

                    </div>



                </div>
            </div>
        </div>
        </div>
    </section>
    <!-- ***** Section 1 End ***** -->


    <!-- ***** Footer Start ***** -->
    <div id="footer"></div>

    <!-- jQuery -->
    <script src="assets/js/jquery-2.1.0.min.js"></script>

    <!-- load in header and footer -->
    <script>
        $(function () {
            $("#header").load("./header.html");
            $("#footer").load("./footer.html");
        });
    </script>

    <!-- Bootstrap -->
    <script src="assets/js/popper.js"></script>
    <script src="assets/js/bootstrap.min.js"></script>

    <!-- Plugins -->
    <script src="assets/js/owl-carousel.js"></script>
    <script src="assets/js/scrollreveal.min.js"></script>
    <script src="assets/js/waypoints.min.js"></script>
    <script src="assets/js/jquery.counterup.min.js"></script>
    <script src="assets/js/imgfix.min.js"></script>

    <!-- Global Init -->
    <script src="assets/js/custom.js"></script>
</body>

</html>